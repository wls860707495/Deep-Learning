# 神经网络和反向传播算法


对应隐藏层的节点的误差项来说，其误差值为
 
       &4 = a4*(1-a4)*(w84*&8 + w94*&9)
       其中(w84*&8 + w94*&9)为输出层所有节点的权重*损失值的和
       
   而输出层的损失值为
   
        &8 = y1*( 1 - y1)*( t1 - y1)
        
 最后更新每个连接上的权值：
 
          wji <-- wji + n * &j * xji (n为学习速率的常数)
          
          
  我们设netj是节点j的加权输入，即：
       
         netj = wj（向量）* xj（向量）
   Ed是netj的函数（因为误差Ed中有着y，而y是由wi*xi得来的）  
   注意反向传播算法的推导（链式求导法则）。如果激活函数不同、误差计算方式不同、网络连接结构不同、优化算法不同，则具体的训练规则也会不一样
   
 ## 对于神经网络中超参数的确定
  
   神经网络中超参数一般都是根据人为经验来确定，其中输入层输出层的神经元数目可以根据训练目标的的不同来进行分类。
              
              比如： 在识别 手写数字是0-9中的哪一个时，我们用28*28的图片来进行训练，其中每个图片的像素点有784个，所以
             我们将输入层的神经元数目设为784，而一共有0-9十个数字，那么我们将输出层设为十个神经元，此时根据输出层结果的
             不同，值最大的那一个节点就是我们想要的分类。
                    而对于隐藏层神经元数目的确定，则是根据经验来判断，此处有经验公式可循（----未写）。
