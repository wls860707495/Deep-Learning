# 卷积神经网络
对于全连接神经网络，其神经元排列为一条直线，计算y的值时使用的是向量。而卷积神经网络，如训练一张图片时，输入为一张图片，通过两个3 * 3的filter，其将会生成两个3 * 3的feature map，这三个feature map即通过卷积层来得到的feature map。   
每个卷积层可以有多个filter，而每个filter中的深度需要和前一个feature map的深度相同。其中卷积层有几个ffilter那么卷积出来就有几个feature map。
## 解决全连接神经网络权重等参数过多以及消除无效的大量权重
局部连接： 这个是最容易想到的，每个神经元不再和上一层的所有神经元相连，而只和一小部分神经元相连。这样就减少了很多参数。  
权值共享： 一组连接可以共享同一个权重，而不是每个连接有一个不同的权重，这样又减少了很多参数。  
下采样： 可以使用Pooling来减少每层的样本数，进一步减少参数数量，同时还可以提升模型的鲁棒性。   
对于图像识别任务来说，卷积神经网络通过尽可能保留重要的参数，去掉大量不重要的参数，来达到更好的学习效果。

     当步幅设置为2的时候，Feature Map就变成2*2了。这说明图像大小、步幅和卷积后的Feature Map大小是有关系的。
## Pooling层

Pooling层主要的作用是下采样，通过去掉Feture Map中不重要的样本，进一步减少参数数量。
```
 eg：Max Pooling--->其逆操作被称为上池化
     即在n*n的样本中取最大值，作为采样后的样本值。--->即在4*4的feature map中，Max Pooling即取左上4个格中的一个有最大值的格，而其余右上、左下、右下可以分别取3个最大的值，将4个值拼成2*2的feature map，这样对于4*4的feature map来说就减少了很多的参数量。
     上池化（unPooling）
     unPooling的过程，特点是在Maxpooling的时候保留最大值的位置信息，之后在unPooling阶段使用该信息扩充Feature Map，除最大值位置以外，其余补0。从图中即可看到两者结果的不同。
     其与Unsampling的区别为：UnSampling阶段没有使用MaxPooling时的位置信息，而是直接将内容复制来扩充Feature Map。第一幅图中右边4*4矩阵，用了四种颜色的正方形框分割为四个区域，每一个区域内的内容是直接复制上采样前的对应信息。
 ```
除Max Pooling外，常用的还有Mean Pooling-->即取各样本的平均值。对于深度为D的Feature Map，各层独立做Pooling，因此Pooling后的深度仍然为D。  

## 互相关和卷积
卷积(convolution)相当于将filter旋转180度的互相关(cross-correlation),互相关就是卷积层的操作-->进行与filter的点乘。
而卷积(convolution)是在进行反向传播算法-->计算误差时（卷积层反向传播时）使用到的方式，记得要补零。  
##  注
权重的更新需要用到神经元的误差值，而误差值的计算算也需要上一个权重。-->为什么要计算误差。

权重项梯度的计算即为使用损失map（sensitivity map）作为卷积核，在input进行互相关（cross-correlation）得到。
而偏置项作为常数值，其梯度就是损失map（sensitivity map）的所有误差项之和。

对于Pooling层  
```
例如：Max Pooling
     下一层的误差项的值会原封不动的传递到上一层对应区块中的最大值所对应的神经元，而其他神经元的误差项的值均为0.
     Mean Pooling
     下一层的误差项的值会平均分配到上一层对应区块中的所有神经元。
      
```
对于误差项&j，其实质上就是网络的损失函数对神经元加权输入的偏导数。  
而对于每个神经元连接权重wji的梯度（wji表示从神经元i连接到神经元j的权重），公式为损失函数对wji的偏导--->ai * &j，其中ai，表示神经元i的输出。
## 总结
1、首模型的建立函数-->y = w * x + b ，激活函数可以自己对其进行选择-->算出y的值来之后对其进行筛选。其中b为偏置项，而w为权重可对其进行初始化。（未确定）  
2、目标函数Ed: 也即损失函数、代价函数。  
```
例如：交叉熵损失函数、平方差损失函数等。--->求真实值与预测值之间差距的函数。 

```
3、除此外还有通过目标函数来对权重w或者是进行调整优化的函数，比如随机梯度下降或者是梯度下降等。可以根据此获取模型参数的最优值。  
## Padding
1、当图像进行卷积核操作时，其图像会越来越小。  
2、一张图片进行卷积核操作时，其角上及边缘的像素点等因为filter仅做过很少的运算，那么这地方的图像特征将提取的很少。  
所以说，当我们想要获取图像边缘的更多信息或者是对卷积后的feature map规定大小时，可以使用padding来填充0人为设定feature map的大小。
## 上采样与下采样--->此处可多看几个方法
1、上采样：即指的是任何可以让图像变成更高分辨率的技术。  
2、下采样：使图片的分辨率变低，池化即下采样的一种。
## 反卷积
这里介绍两种反卷积的做法：  
1、即与使用full卷积的原理类似，使图片的分辨率扩大。  
2、另外一种反卷积做法，假设原图是3X3，首先使用上采样让图像变成7X7，可以看到图像多了很多空白的像素点。使用一个3X3的卷积核对图像进行滑动步长为1的valid卷积，得到一个5X5的图像，我们知道的是使用上采样扩大图片，使用反卷积填充图像内容，使得图像内容变得丰富，这也是CNN输出end to end结果的一种方法。


