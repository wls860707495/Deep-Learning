# 吴恩达公开课
## 损失函数
衡量的是在单个样本上的表现
## 成本函数
衡量的是在整体样本上的表现，即将所有训练样本损失函数和相加并平均。（被定义为平均值）
## Tensorflow中的padding中的‘SAME’和‘VALID’
其中对于‘SAME’：n(output) = n(input)/S   -->s为1   经卷积后输出：n(图像大小)+2 * p(填充像素大小)-f(filter大小)+1
对于‘VALID’: n(output) = (n(input)-f+1)/S   -->s为1  经卷积后输出：n(图像大小)-f(filter大小)+1
![rongqi](https://github.com/wls860707495/Deep-Learning/blob/master/img/output.png)
## 数据集
### 训练集（所用数据量最大）
对模型进行训练。
### 验证集
验证不同的算法，测试那种算法更行之有效
### 测试集（用少量数据）
测试模型的效果
## 偏差与方差
### 方差(数据过拟合)
通过查看训练集误差和验证集误差，便可以诊断一个算法是否具有高方差。
### 偏差(数据欠拟合) 
训练集的拟合程度不高，错误率较高，且验证集中的错误率与训练集中错误率相差不多。
## 优化算法
### mini-batch梯度下降算法
训练集分为很多小的子集（mini-batch），每个子集中都含有很多的样本   
```
训练集较小：batch梯度下降法（<2000）
训练集较大：mini-batch（mini-batch大小一般设置为64，128，256，512）
```
### 指数加权平均
将除了当天之外的其它天的数据进行加权，权重大小不一样得到的效果便不一样。
### 动量梯度下降法
两个超参数，用β控制着指数加权平均数来对dW和db进行处理，之后通过设置α对权重以及偏置项进行更新，能够最小化碗装函数，
### RMSprop(均方根)
此处用的是微分平方的加群平均数，即相比于动量梯度下降法，dW变为了(dW)^2，并且对于W和b的更新公式亦有转变，公式如下
![rongqi](https://github.com/wls860707495/Deep-Learning/blob/master/img/RMSprop.png)
### Adam优化算法（Momentum与RMSprop结合）
结合Momentum与RMSprop，与其不同的是，算出两算法相应的Vdw以及Sdw等参数后，需要计算其各自的修正值，之后根据修正值来更新W与b
![rongqi](https://github.com/wls860707495/Deep-Learning/blob/master/img/adam.png)
## 对于正则化（L1,L2）
无论怎样，其类似于给权重进行1-α* (λ/m) 得衰减，所以又叫作权重衰减。  
为什么会减少过拟合（原因）：1、直观上理解就是，当w被设置为接近与0得值，而设置的λ较大时，会基本消除这些隐藏单元的影响（实际上这些隐藏单元还在）。2、当λ较大时，w会较小，那么Z的值会几乎处于一个类线性区间内，那么这样简单的类线性网络会很少发生过拟合。
### 学习率衰减
通过衰减学习率，可以在训练初期使梯度以较大步伐进行下降，并且在收敛时不会出现幅度较大的情况。
### BatchNorm
作用：使隐藏单元值得均值和方差标准化
![rongqi](https://github.com/wls860707495/Deep-Learning/blob/master/img/BatchNorm.png)
## 如何通过卷积提取的边缘
当我们进行卷积运算时，对于一张图片上不同的灰度值，表现出来得明亮是不一样的，通过filter生成的特征图中的每个灰度值亦不同，由此特出的特征图即为表现出的边缘亮度等。
## LeNet-5
![rongqi](https://github.com/wls860707495/Deep-Learning/blob/master/img/LeNet-5.png)
## AlexNet
![rongqi](https://github.com/wls860707495/Deep-Learning/blob/master/img/AlexNet.png)
## VGG-16
![rongqi](https://github.com/wls860707495/Deep-Learning/blob/master/img/VGG-16.png)
## ResNet残差网络（跳跃连接）
有助于解决梯度消失和梯度爆炸
![rongqi](https://github.com/wls860707495/Deep-Learning/blob/master/img/ResNet.png)
架构图：
![rongqi](https://github.com/wls860707495/Deep-Learning/blob/master/img/ResNet2.png)
## N in N
即应用1x1卷积的方法。
## 对象定位
对于一个目标分类的情况来讲，一个数y据集的训练集中不能够只有给出它的标签，还要有边界框坐标以及它的宽和高。那么其对于最后的输出不仅要有各个类别标签的概率，还要有这个被分类出的图像的边框高度和宽度以及它的中心点的坐标。  
其中，对于y的输出类别：
![rongqi](https://github.com/wls860707495/Deep-Learning/blob/master/img/classify-y.png)
c1、c2、c3分别是pedestrian、car以及motorcycle的类别，Pc表示是1、2、3类还是背景类。
## 特征点识别
在神经网络最后一层多输出几个个值（眼角的边界坐标）.   
```
比如要输出人脸特征时，输出y的那些向量y = [q，l1，l2，l3...]即q表示是否为脸，l1、l2等表示其各个特征点。
```
## 对象检测
将剪切好的图像并作为标签送入卷积神经网络进行训练，之后使用滑动窗口来对图像的特点进行识别并输出0-1分类。
## 循环序列模型
### RNN
对于使用RNN进行自然语言处理，如果我们x<1>和a<0>的输入均为0向量，那么它的输出y<1>的softmax则指的是所有词典中的词作为第一个词的概率。后边预测的则是相应的考虑前边的输出之后的softmax在词典中所有词的概率。
![rongqi](https://github.com/wls860707495/Deep-Learning/blob/master/img/RNN.png)
### GRU
加入了门控单元，防止梯度消失，使后边的预测能更好的与前边的内容相关联。在下图中，c<t>即维细胞记住的状态，其中sigmoid得出的结果（即u）用来判断对于得出的c~<t>保留多少，以及c<t-1>需要保留多少。因为u是一个很多维的参数，所以它决定着很多的记忆细胞（可以理解为单词的各种属性，比如是单数还是复数，含义是食物还是玩具）哪个在每个时间步上要做更新。
![rongqi](https://github.com/wls860707495/Deep-Learning/blob/master/img/GRU.png)
下图为正式版本的GRU，相对于上图的GRU单元，多出了门控单元r，这个单元表示的是c~<t>与c<t>有多大的相关联性。
![rongqi](https://github.com/wls860707495/Deep-Learning/blob/master/img/GRU2.png)
### LSTM
LSTM与GRU类似，其中GRU的更新门实现了更新和遗忘。而对于LSTM网络来说，它用了两个门，分别用来实现更新和遗忘。并且参数比GRU多出了a<t>（RNN中应的a<t> --> 此参数在GRU中被c<t>代替，即在GRU中c<t>=a<t>）
![rongqi](https://github.com/wls860707495/Deep-Learning/blob/master/img/LSTM.png)
### 双向RNN
对于双向RNN，我们可以考虑预测的某个词的前后关系，那么比如在应用到语音识别时，我们需要等人说完整个句子才能对此进行识别。
![rongqi](https://github.com/wls860707495/Deep-Learning/blob/master/img/RNN2.png)
## 自然语言处理与词嵌入
### 词嵌入
例如，每个词有300个特征，我们根据这300维的特征，将词嵌入到一个300维的空间当中，这叫做词嵌入。其中特征就是它的词汇表征。
![rongqi](https://github.com/wls860707495/Deep-Learning/blob/master/img/word.png)
当我们知道了一个词对应另一个词（如man对应wuman），通过此想知道另一个词对应于什么（如king该对应什么），我们查看这两个向量间的距离（余弦相似度、平方距离、欧氏距离等）是否和已知的相似，具体可以看上图。
### 词处理
我们在对词进行处理时，首先将词转换为one-hot向量，再经过embedding层与嵌入矩阵相乘，最终的到的向量即为词表征的向量表示。
### 关于词的处理
仅用softmax层在词典有大量词汇的情况下，计算会非常复杂，我们如果使用分级的softmax分类器，那么我们可以首先判断其在整个词典中的哪一部分，再据此进行计算，会大大减少计算量。


为什么filter和原图的相似程度越大，其卷积的输出就越大？？？？

