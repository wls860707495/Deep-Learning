# 吴恩达公开课
## 损失函数
衡量的是在单个样本上的表现
## 成本函数
衡量的是在整体样本上的表现，即将所有训练样本损失函数和相加并平均。（被定义为平均值）
## Tensorflow中的padding中的‘SAME’和‘VALID’
其中对于‘SAME’：n(output) = n(input)/S   -->s为1   经卷积后输出：n(图像大小)+2 * p(填充像素大小)-f(filter大小)+1
对于‘VALID’: n(output) = (n(input)-f+1)/S   -->s为1  经卷积后输出：n(图像大小)-f(filter大小)+1
![rongqi](https://github.com/wls860707495/Deep-Learning/blob/master/img/output.png)
## 数据集
### 训练集（所用数据量最大）
对模型进行训练。
### 验证集
验证不同的算法，测试那种算法更行之有效
### 测试集（用少量数据）
测试模型的效果
## 偏差与方差
### 方差(数据过拟合)
通过查看训练集误差和验证集误差，便可以诊断一个算法是否具有高方差。
### 偏差(数据欠拟合) 
训练集的拟合程度不高，错误率较高，且验证集中的错误率与训练集中错误率相差不多。
## 优化算法
### mini-batch梯度下降算法
训练集分为很多小的子集（mini-batch），每个子集中都含有很多的样本   
```
训练集较小：batch梯度下降法（<2000）
训练集较大：mini-batch（mini-batch大小一般设置为64，128，256，512）
```
### 指数加权平均
将除了当天之外的其它天的数据进行加权，权重大小不一样得到的效果便不一样。
### 动量梯度下降法
两个超参数，用β控制着指数加权平均数来对dW和db进行处理，之后通过设置α对权重以及偏置项进行更新，能够最小化碗装函数，
### RMSprop(均方根)
此处用的是微分平方的加群平均数，即相比于动量梯度下降法，dW变为了(dW)^2，并且对于W和b的更新公式亦有转变，公式如下
![rongqi](https://github.com/wls860707495/Deep-Learning/blob/master/img/RMSprop.png)
### Adam优化算法（Momentum与RMSprop结合）
结合Momentum与RMSprop，与其不同的是，算出两算法相应的Vdw以及Sdw等参数后，需要计算其各自的修正值，之后根据修正值来更新W与b
![rongqi](https://github.com/wls860707495/Deep-Learning/blob/master/img/adam.png)
### 学习率衰减
通过衰减学习率，可以在训练初期使梯度以较大步伐进行下降，并且在收敛时不会出现幅度较大的情况。
### BatchNorm
作用：使隐藏单元值得均值和方差标准化
![rongqi](https://github.com/wls860707495/Deep-Learning/blob/master/img/BatchNorm.png)
## 如何通过卷积提取的边缘
当我们进行卷积运算时，对于一张图片上不同的灰度值，表现出来得明亮是不一样的，通过filter生成的特征图中的每个灰度值亦不同，由此特出的特征图即为表现出的边缘亮度等。




