# 循环神经网络
当我们使用卷积神经网络以及简单的全连接神经网络时，单个神经元的的输入与上一个神经元的输入并没有关系。但是我们做的一些任务需要处理一些序列。   
     
     比如：理解一句话的意思需要将所有的词连接起来。
除此之外，我们在处理视频序列时也不应只是单独分析每一帧    
其最先用于语言模型-->自然语言处理 中。  
## 语言模型
语言模型是对一种语言的特征进行建模，它有很多很多用处。比如在语音转文本(STT)的应用中，声学模型输出的结果，往往是若干个可能的候选词，这时候就需要语言模型来从这些候选词中选择一个最可能的。当然，它同样也可以用在图像到文本的识别中(OCR)。   
对于对自然语言进行处理，一般均是以向前（或向后）看多少个词来预测需要填什么。而RNN理论上可以向前（或向后）看任意个词。  

## 对于循环神经网络
循环神经网络的隐藏层的值s不仅仅取决于当前这次的输入x，还取决于上一次隐藏层的值s。权重矩阵 W就是隐藏层上一次的值作为这一次的输入的权重。  
```
                              Ot = g(V st) ---> 式1
                              st = f(U xt + W st-1) ---> 式2
```
式1是输出层的计算公式，输出层是一个全连接层，也就是它的每个节点都和隐藏层的每个节点相连。V是输出层的权重矩阵，g是激活函数。式2是隐藏层的计算公式，它是循环层。U是输入x的权重矩阵，W是上一次的值作为这一次的输入的权重矩阵，f是激活函数。  

从上面的公式我们可以看出，循环层和全连接层的区别就是循环层多了一个权重矩阵 W。  
将式2带入式1可知：，循环神经网络的输出值ot，是受前面历次输入值xt、xt-1、xt-2、xt-3、...影响的，这就是为什么循环神经网络可以往前看任意多个输入值的原因。  
## 双向循环网络
如图（图未画），双向卷积神经网络的隐藏层要保存两个值，一个A参与正向计算，另一个值A'参与反向计算。最终的输出值y2取决于A和A'。  
仿照式1和式2，可以写出双向循环神经网络的计算方法：  
```
                            Ot = g(V st + V' s't)
                            st = f(U xt + W st-1)
                            s't = f(U' xt + W' s't+1)
```
从上面三个公式可以看出，正向和反向计算不共享权重，也就是说U和U'、W和W'、V和V'都是不同的权重矩阵。  
## 深度循环神经网络
即堆叠两个以上隐藏层的深度循环神经网络。
## 循环神经网络的训练
### 循环神经网络的训练算法：BPTT
BPTT算法是针对循环层的训练算法，它的基本原理和BP算法是一样的，也包含同样的三个步骤：  

1、前向计算每个神经元的输出值；  
2、反向计算每个神经元的误差项值，它是误差函数E对神经元j的加权输入的偏导数；  
3、计算每个权重的梯度。  
最后再用随机梯度下降算法更新权重。
#### 误差项的计算
BTPP算法将第l层t时刻的误差项&(l t)值沿两个方向传播，一个方向是其传递到上一层网络，得到&(l-1 t)，这部分只和权重矩阵U有关；另一个是方向是将其沿时间线传递到初始t1时刻，得到&l1，这部分只和权重矩阵W有关。  
对公式(未注出-->后加)进行总结可知其规律，并可以根据此规律求出任意时刻的误差项&k。  
循环层的加权输入netl与上一层的加权输入netl-1关系如下:
```
                         net(l t) = Ua(l-1 t) + Wst-1
                         a(l-1 t) = fl-1(net(l-1 t))
```
上式中net(l t)是第l层神经元的加权输入(假设第l层是循环层)：net(l-1 t)是第l-1层神经元的加权输入；a(l-1 t)是第l-1层神经元的输出；f(l-1)是第l-1层的激活函数。 由以上两个公式可以算出将误差传递到上一层的算法(未注出-->后加)。

    即对于BTPP误差项的传播可以有两个方向，其一是沿时间线向前传播，其二是将误差由本层向上一层进行传播。
#### 权重梯度的计算
对于循环层梯度的计算，最终的梯度▽wE是各个时刻的梯度之和。-->推导过程未看，后加
上为计算出的权重矩阵W的梯度，有了梯度即可对权重进行更新。  
同权重矩阵W类似，我们可以同样计算出权重矩阵U的计算方法。与权重矩阵W一样，U的最终梯度也为各个时刻的梯度之和。
### RNN的梯度爆炸和消失问题
产生原因：当误差项沿时间线向前进行传播时，根据误差项的公式可以知道推出一带绝对值的公式，而推导出的公式是一个指数函数，t-k很大，即向前看很远时，会导致对应的误差项的值增长或缩小的非常快，这样就会导致相应的梯度爆炸和梯度消失的问题。  
通常来说，梯度爆炸更容易处理一些。因为梯度爆炸的时候，我们的程序会收到NaN错误。我们也可以设置一个梯度阈值，当梯度超过这个阈值的时候可以直接截取。  

梯度消失更难检测，而且也更难处理一些。总的来说，我们有三种方法应对梯度消失问题：  

合理的初始化权重值。初始化权重，使每个神经元尽可能不要取极大或极小值，以躲开梯度消失的区域。  
使用relu代替sigmoid和tanh作为激活函数。原理请参考上一篇文章零基础入门深度学习(4) - 卷积神经网络的激活函数一节。  
使用其他结构的RNNs，比如长短时记忆网络（LTSM）和Gated Recurrent Unit（GRU），这是最流行的做法。我们将在以后的文章中介绍这两种网络。  
## 向量化
即神经网络的输入和输出都是向量，为了让语言模型能够被神经网络处理，我们必须把词表达为向量的形式，这样神经网络才能处理它。   
神经网络的输入是词，我们可以用下面的步骤对输入进行向量化：  

1、建立一个包含所有词的词典，每个词在词典里面有一个唯一的编号。  
2、任意一个词都可以用一个N维的one-hot向量来表示。其中，N是词典中包含的词的个数。假设一个词在词典中的编号是i，v是表示这个词的向量，vj是向量的第j个元素，则：
```
                         vj = 1, j=i
                         vj = 0, j!=i
```
使用这种向量化方法，我们就得到了一个高维、稀疏的向量（稀疏是指绝大部分元素的值都是0）。处理这样的向量会导致我们的神经网络有很多的参数，带来庞大的计算量。因此，往往会需要使用一些降维方法，将高维的稀疏向量转变为低维的稠密向量。  
而语言模型要求的输出是下一个最可能的词，我们可以让循环神经网络计算计算词典中每个词是下一个词的概率，这样，概率最大的词就是下一个最可能的词。因此，神经网络的输出向量也是一个N维向量，向量中的每个元素对应着词典中相应的词是下一个词的概率。  
## Softmax层
语言模型是对下一个词出现的概率进行建模。那么，怎样让神经网络输出概率呢？方法就是用softmax层作为神经网络的输出层。  
softmax layer的输入是一个向量，输出也是一个向量，两个向量的维度是一样的（在这个例子里面是4）。输入向量x=[1 2 3 4]经过softmax层之后，经过上面的softmax函数计算，转变为输出向量y=[0.03 0.09 0.24 0.64]。--->计算过程未写，后加。  
我们来看看输出向量y的特征：  

1、每一项为取值为0-1之间的正数；  
2、所有项的总和是1。  
我们不难发现，这些特征和概率的特征是一样的，因此我们可以把它们看做是概率。对于语言模型来说，我们可以认为模型预测下一个词是词典中第一个词的概率是0.03，是词典中第二个词的概率是0.09，以此类推。  
## 语言模型的训练
首先，我们获取输入-标签对：

输入	标签  
s	我  
我	昨天  
昨天	上学  
上学	迟到  
迟到	了  
了	e 
然后，使用前面介绍过的向量化方法，对输入x和标签y进行向量化。这里面有意思的是，对标签y进行向量化，其结果也是一个one-hot向量。例如，我们对标签『我』进行向量化，得到的向量中，只有第2019个元素的值是1，其他位置的元素的值都是0。它的含义就是下一个词是『我』的概率是1，是其它词的概率都是0。  
  
最后，我们使用交叉熵误差函数作为优化目标，对模型进行优化。  
## 交叉熵误差
一般来说，当神经网络的输出层是softmax层时，对应的误差函数E通常选择交叉熵误差函数--->定义公式未写，后加   
一般来说，对概率进行建模时，使用交叉熵误差更好。  
## 总结
1、循环神经对于卷积神经网络来说考虑了在一层的神经元之间的关系连接，即可以向前向后看，其神经元的值st会与st-1有关-->即同同一层的时间顺序上的一个神经元有关。除此之外，其当然与上一层的输入值有关。对于这两个相关因素，均有权重矩阵对其进行权重分配。  
2、对于循环神经网络来说，其误差值的计算有两种，即根据这两个误差值来对两个不同的权重进行更新。  
3、循环网络输入层进行输入时，比如对语义进行处理时，输入的词语一般会处理为向量形式。所以对于计算st时其W与U矩阵为 二维权重矩阵 对输入值xt与st-1进行计算。




