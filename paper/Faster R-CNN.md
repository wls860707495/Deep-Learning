# Faster R-CNN
Faster R-CNN分为两个模块，其中第一个模块为RPN网络，第二个模块为RoI pooling。即我们通过RPN（全卷积神经网络）来在特征图上提取proposals，然后再使用通过Fast R-CNN检测器对目标进行检测。
## FCN
### 全连接层的替换
FCN的核心在于将CNN最后的全连接层设置为1 x 1的 卷积层（即1x1x4096,1x1x4096,1x1x1000）,、直接输出目标物体所属的像素范围。  
### 层与层之间的跳跃结构（目的是为了得到特征图更加细节的部分）
在FCN的结构之中，我们有很多的池化层，包括pool1，pool2，pool3，pool4，pool5等，原本的图像在我们经过conv1，pool1之后，图像变味了原来大小的1/2，同样，在pool2，pool3，pool4，pool5后图像分别变为了原来大小的1/4，1/8，1/16，1/32。此时我们将pool3、pool4、conv7后得到的特征图(在经过conv7之后的这一层得到的图像不叫特征图叫heatMap)保存下来这三层得到的特征图大小不一样，而且在这上面保存出来的三个特征图都包含了图像的不同细节。之后我们将得到的三个特征图分别使用反卷积的方式得到三个与原图大小相同的图像（三个图像分别为conv7后的原图、pool4后的图与conv7后的图融合后的图、pool3+pool4+conv7得到的图）。
### 逐像素预测
我们使用反卷积将最后一个卷积层的feature map进行上采样，使这个feature map恢复至与输入图像相同的尺寸，从而可以对每个像素都产生了一个预测，同时保留了原始输入图像中的空间信息，最后在上采样的特征图上进行逐像素分类。之后逐个像素计算softmax分类的损失，相当于每一个像素对应一个训练样本。  
如何通过这三个卷积得到的每个类别的得分：
```
如果我们想让224×224尺寸的浮窗，以步长为32在384×384的图片上滑动，把每个经停的位置都带入卷积网络，最后得到6×6个位置的类别得分。上述的把全连接层转换成卷积层的做法会更简便。如果224×224的输入图片经过卷积层和下采样层之后得到了[7x7x512]的数组，那么，384×384的大图片直接经过同样的卷积层和下采样层之后会得到[12x12x512]的数组。然后再经过上面由3个全连接层转化得到的3个卷积层，最终得到[6x6x1000]的输出((12 – 7)/1 + 1 = 6)。这个结果正是浮窗在原图经停的6×6个位置的得分！
```
### 网络的损失
在FCN中，网络输入的batchsize为1，也就是说，因为是对逐像素计算损失的，每个像素有一个loss，即每个像素点都是一个分类任务，一个图像就有对应像素点个样本，所以FCN分割任务的batch是一个图片，之后在所有像素点上的分类loss加起来计算一次梯度更新。
### FCN中的结论
在对图像进行反卷积融合时，先从深层的特征图开始融合，然后向外一步步融合（即先融合pool4-->在这里，上采样也进行了三次，每次上采样大小等于将要融合的pool层输出特征之的大小，再融合pool3），这样的效果比一步上采样到位效果要好。
## RPN（区域生成网络） -->  Faster R-CNN的核心
RPN网络的作用是输入一张图像，输出的是一批区域候选区域。其中它的基础网络是由ZF-5和VGG网络构成，在基础网络生成了feature map之后，我们使用特出的sliding windows以及anchor boxs的方式生成锚点框（ROIpooling的候选区域）
### 特征图处理详解
对于ZF-5得到的feature map，采用n * n的方式进行sliding window滑动处理（文中所用的滑动窗口的大小为3 * 3),然后通过1 * 1的卷积网络输出两路，分别用于分类和回归。上图为sliding window的详细处理过程。
 ![rongqi](https://github.com/wls860707495/Deep-Learning/blob/master/img/feature_map_deal.png)
其中：  
256-d ：ZF-5的最后一层输出filter的个数为256，因此每一个滑动窗口输出是一个256维的向量。  
k：每一个滑动窗口的中心位置，预测k个候选区，这些候选区被称为anchor。
2k scores和4k oordinates：其中2k使用为分类层的输出目标为foreground和background的概率，4k则是每个anchor box包含4个位置坐标。  
以下为得到类别分数和坐标的详解：
```
··· k为单个位置对应的anchor的个数，此时k=9，通过增加一个3×3滑动窗口操作以及两个卷积层完成区域建议功能；

··· 一个卷积层将特征图每个滑窗位置编码成一个特征向量，第二个卷积层对应每个滑窗位置输出k个区域得分，表示该位置的anchor为物体的概率，这部分总输出长度为2×k(一个anchor对应两个输出：是物体的概率+不是物体的概率)和k个回归后的区域建议(框回归)，一个anchor对应4个框回归参数，因此框回归部分的总输出的长度为4×k，并对得分区域进行非极大值抑制后输出得分Top-N(文中为300)区域，告诉检测网络应该注意哪些区域，本质上实现了Selective Search、EdgeBoxes等方法的功能。
```
对应上图的解释：
 ![rongqi](https://github.com/wls860707495/Deep-Learning/blob/master/img/feature_map_deal_ex.png)
### 关与anchor（锚点）
在ZF-5输出的feature map上进行滑窗操作，是为了获取滑窗对应的感受野内是否存在目标（概率，以及目标的位置坐标），在 ZF-5 输出的 feature map 上进行滑窗操作，是为了获取滑窗对应的感受野内是否存在目标（概率，以及目标的位置坐标），由于目标大小和长宽比例可能不一，因此需要多个窗口。Anchor 的机制是，以滑动窗的中心为采样点，采用不同的 scale（规模，像素点的个数） 和 ratio（比例，即长宽比），生成不同大小共 k 个 anchor 窗口。对于一个W * H小的 feature map 来说，生成的 anchor 总数为WxHxk，例如scale 为（128x128，256x256，512x512），ratio 为（1:2，1:1，2:1）时，则生成 9 种 anchor。--> 这九个anchor是相对于原图来说的，也就是说，这九个anchor的大小和长宽比例是在原图像上的。
采用 Anchor 机制能够从整图的 feature map 上直接提取候选区域（映射到原图）及其特征，相比较 R-CNN 和 Fast R-CNN 中 selective search（或EdgeBoxes） 的方法，避免了大量的额外运算，且整个过程融合到一个网络中，方便训练和测试。  
我们学习到的k个回归框并不会共享权重，每个回归器负责一个大小不同比例的锚点框。
### RPN损失函数的训练
### 框回归
对于RPN的损失函数，我们得到的anchor boxs为候选区域 --> 设置为A ，但是anchor boxs（A）与真实框（G）之间还有很大的差距，我们的loss通过一个线性变换将
anchor boxs（A）变换为真实框G‘，使得预测框G'能够近似等于真实框G。此时我们的loss函数由此得来，即d（A） = w^T * f(A),其中d(A)是由anchor boxs（A）通过线性变换得到的预测值，而我们的loss函数目的即让预测值dx（A）、dy（A）、dw（A）、dh（A）与真实值差距tx、ty、tw、th最小，代价函数略（目的即学习参数w使得损失函数最小），其中tx、ty、tw、th公式如下：
```
tx = (Gx - Ax)/Aw
ty = (Gy - Ay)/Ah
tw = log(Gw / Aw)
th = log(Gh / Ah)
```
### 候选框修正
目的是为了得到每一个anchor A的修正参数dx（A）、dy（A）、dw（A）、dh（A），如此我们可以计算出精确的anchor，之后按照物体的区域得分从大到小对得到的anchor排序，然后提出一些宽或者高很小的anchor(获取其它过滤条件)，再经过非极大值抑制抑制，取前Top-N的anchors，然后作为proposals(候选框)输出，送入到RoI Pooling层。
此处RPN网络的出的候选框的分数（只有两类）
## 分类和框回归
通过RoI Pooling层我们得到所有候选区组成的特征向量，然后送入全连接层和softmax计算每个候选框具体属于哪个类别，输出类别的得分；同时再次利用框回归获得每个候选框相对于实际位置的偏移量预测值，用于对候选框进行修正，得到更精确的目标检测框。
## Faster R-CNN训练方式
Faster R-CNN在使用RPN生成候选框之后，剩下的网络结构和Fast R-CNN一模一样。在训练时，需要训练两个网络，一个是RPN网络，一个是得到框之后的分类网络。通常的方法是交替训练，即在一个batch内，先训练RPN网络一次，再训练分类网络一次。
## 注
1、在得到特征图之后，进行3x3的卷积，但是其特征图的大小不变，而这样特征图的每个像素点都融合了周围8个点的特征（ --> 可能为了增加每个点的鲁棒性）  
2、在框回归中，预测框是由anchor boxs的值通过变换得来的。  
3、在Faster R-CNN中Fast R-CNN以及RPN网络具有共享的卷积层，论文中使用的是交替训练的方式，首先使用RPN网络提取出proposal --> 这时对RPN完了过进行训练，之后通过提取出的proposal放入Fast R-CNN中进行训练（此时两个网络并不共享卷积层）。下一步使用Faster R-CNN的网络权重对RPN进行初始化（这一步两个网络共享卷积层）。之后可以不断地进行交替训练。  








