# 彩色超声检测系统中结合成本敏感学习的改进型深度学习网络在卵巢癌早期检测中的应用 
## 旋转不变的均匀二值模式（ULBP）

## 归一化
```
  把几个数量级不同的数据，放在一起比较（或者画在一个数轴上），比如：一条河的长度几千甚至上万km，与一个人的高度1.7m，放在一起，人的高度几乎可以被忽略，所以为了方便比较，缩小他们的差距，但又能看出二者的大小关系，可以找一个方法进行转换。

  另外，在多分类预测时，比如：一张图，要预测它是猫，或是狗，或是人，或是其它什么，每个分类都有一个预测的概率，比如是猫的概率是0.7，狗的概率是0.1，人的概率是0.2... , 概率通常是0到1之间的数字，如果我们算出的结果，不在这个范围，比如：700，10，2 ，甚至负数，这样就需要找个方法，将其转换成0-1之间的概率小数，而且通常为了满足统计分布，这些概率的和，应该是1。
  一般神经网络中的归一化均使用softmax进行。
```
## 随机森林（RF）
参考于：https://blog.csdn.net/qq_34106574/article/details/82016442  
随机森林实质上是由多棵决策树组成，其中每棵决策树均对输入做出决策，最后将投票数最多的作为依据。  
例如
```
    森林中召开会议，讨论某个动物到底是老鼠还是松鼠，每棵树都要独立地发表自己对这个问题的看法，也就是每棵树都要投票。
  
  该动物到底是老鼠还是松鼠，要依据投票情况来确定，获得票数最多的类别就是森林的分类结果。森林中的每棵树都是独立的，
  
  99.9%不相关的树做出的预测结果涵盖所有的情况，这些预测结果将会彼此抵消。少数优秀的树的预测结果将会超脱于芸芸“噪音”
  
  ，做出一个好的预测。将若干个弱分类器的分类结果进行投票选择，从而组成一个强分类器，这就是随机森林bagging的思想
  
  （关于bagging的一个有必要提及的问题：bagging的代价是不用单棵决策树来做预测，具体哪个变量起到重要作用变得未知，
  
  所以bagging改进了预测准确率但损失了解释性。）。
```
```
　每棵树的按照如下规则生成：

　　1）如果训练集大小为N，对于每棵树而言，随机且有放回地从训练集中的抽取N个训练样本（这种采样方式称为bootstrap sample方法），作为该树的训练集；

　　从这里我们可以知道：每棵树的训练集都是不同的，而且里面包含重复的训练样本（理解这点很重要）。

　　为什么要随机抽样训练集？（add @2016.05.28）

　　如果不进行随机抽样，每棵树的训练集都一样，那么最终训练出的树分类结果也是完全一样的，这样的话完全没有bagging的必要；

　　为什么要有放回地抽样？（add @2016.05.28）

　　我理解的是这样的：如果不是有放回的抽样，那么每棵树的训练样本都是不同的，都是没有交集的，这样每棵树都是"有偏的"，都是绝对"片面的"（当然这样说可能不对），也就是说每棵树训练出来都是有很大的差异的；而随机森林最后分类取决于多棵树（弱分类器）的投票表决，这种表决应该是"求同"，因此使用完全不同的训练集来训练每棵树这样对最终分类结果是没有帮助的，这样无异于是"盲人摸象"。

　　2）如果每个样本的特征维度为M，指定一个常数m<<M，随机地从M个特征中选取m个特征子集，每次树进行分裂时，从这m个特征中选择最优的；

　　3）每棵树都尽最大程度的生长，并且没有剪枝过程。

　　一开始我们提到的随机森林中的“随机”就是指的这里的两个随机性。两个随机性的引入对随机森林的分类性能至关重要。由于它们的引入，使得随机森林不容易陷入过拟合，并且具有很好得抗噪能力（比如：对缺省值不敏感）。
```
随机森林分类效果（错误率）与两个因素有关：
```
    森林中任意两棵树的相关性：相关性越大，错误率越大；
    森林中每棵树的分类能力：每棵树的分类能力越强，整个森林的错误率越低。
```
## softmax
参考于：https://blog.csdn.net/bitcarmanlee/article/details/82320853
softmax简单理解
```
首先我们简单来看看softmax是什么意思。顾名思义，softmax由两个单词组成，其中一个是max。对于max我们都很熟悉，比如有两个变量a,b。如果a>b，则max为a，反之为b。用伪码简单描述一下就是 if a > b return a; else b。 
另外一个单词为soft。max存在的一个问题是什么呢？如果将max看成一个分类问题，就是非黑即白，最后的输出是一个确定的变量。更多的时候，我们希望输出的是取到某个分类的概率，或者说，我们希望分值大的那一项被经常取到，而分值较小的那一项也有一定的概率偶尔被取到，所以我们就应用到了soft的概念，即最后的输出是每个分类被取到的概率。
```
softmax定义
```
对于每一个值，以e为底，每个值作为指数，例如输入有3、1、-3，那么其分别的变形值为e^3、e^1、e^-3,其分别的概率为这几个变形值分别除它们的累加和。
                                                     
                                                    公式为：Si=ei∑jej
```
对于softmax loss function
```
      损失函数越大，说明该分类器在真实标签上的分类概率越小，性能也就越差。
      当损失函数接近正无穷时表明训练发散，需要调小学习速率。
```
## 交叉熵
```
  给定一个策略, 交叉熵就是在该策略下猜中颜色所需要的问题的期望值。更普遍的说，交叉熵用来衡量在给定的真实分布下，使用非真实分布所指定的策略消
  
除系统的不确定性所需要付出成本的大小。交叉的字面意思在于：真实分布与非真实分布的交叉。即表示预测出来的答案与正确答案之间的错误程度。给定一个

方案, 越优的策略, 最终的交叉熵越低。具有最低的交叉熵的策略就是最优化策略，也就是上面定义的熵。因此, 在机器学习中, 我们需要最小化交叉熵。

```
数学上来讲, 交叉熵就是
```
  其中, p 是真正的概率, 例如例子二中, 橘色和绿色是 1/8, 红色是 1/4, 蓝色是 1/2。p^是错误地假设了的概率, 例如, 在例子二中我们错误地假设了所有
  
的颜色的概率都是 1/4。p和 p^ 可能有点容易混淆. 记住一点, log是用来计算在 你的策略下猜中所需要的问题数, 因此, log中需要的是你的预测概率p^。在

决策树中, 如果建立的树不是最优的, 结果就是对于输出的概率分布的假设是错误地, 导致的直接结果就是交叉熵很高。交叉熵不仅仅应用在决策树中, 在其他的

分类问题中也有应用。
```
## 
