# 彩色超声检测系统中结合成本敏感学习的改进型深度学习网络在卵巢癌早期检测中的应用 
## 归一化
　　把几个数量级不同的数据，放在一起比较（或者画在一个数轴上），比如：一条河的长度几千甚至上万km，与一个人的高度1.7m，放在一起，人的高度几乎可以被忽略，所以为了方便比较，缩小他们的差距，但又能看出二者的大小关系，可以找一个方法进行转换。

　　另外，在多分类预测时，比如：一张图，要预测它是猫，或是狗，或是人，或是其它什么，每个分类都有一个预测的概率，比如是猫的概率是0.7，狗的概率是0.1，人的概率是0.2... , 概率通常是0到1之间的数字，如果我们算出的结果，不在这个范围，比如：700，10，2 ，甚至负数，这样就需要找个方法，将其转换成0-1之间的概率小数，而且通常为了满足统计分布，这些概率的和，应该是1。
  一般神经网络中的归一化均使用softmax进行。

## 随机森林（RF）
随机森林实质上是由多棵决策树组成，其中每棵决策树均对输入做出决策，最后将投票数最多的作为依据。  
例如
```
    森林中召开会议，讨论某个动物到底是老鼠还是松鼠，每棵树都要独立地发表自己对这个问题的看法，也就是每棵树都要投票。
  
  该动物到底是老鼠还是松鼠，要依据投票情况来确定，获得票数最多的类别就是森林的分类结果。森林中的每棵树都是独立的，
  
  99.9%不相关的树做出的预测结果涵盖所有的情况，这些预测结果将会彼此抵消。少数优秀的树的预测结果将会超脱于芸芸“噪音”
  
  ，做出一个好的预测。将若干个弱分类器的分类结果进行投票选择，从而组成一个强分类器，这就是随机森林bagging的思想
  
  （关于bagging的一个有必要提及的问题：bagging的代价是不用单棵决策树来做预测，具体哪个变量起到重要作用变得未知，
  
  所以bagging改进了预测准确率但损失了解释性。）。
```

　每棵树的按照如下规则生成：
```
　　1）如果训练集大小为N，对于每棵树而言，随机且有放回地从训练集中的抽取N个训练样本（这种采样方式称为bootstrap sample方法），作为该树的训练集；
```
　　从这里我们可以知道：每棵树的训练集都是不同的，而且里面包含重复的训练样本（理解这点很重要）。  

　　为什么要随机抽样训练集？   

　　如果不进行随机抽样，每棵树的训练集都一样，那么最终训练出的树分类结果也是完全一样的，这样的话完全没有bagging的必要；

　　为什么要有放回地抽样？  

　　我理解的是这样的：如果不是有放回的抽样，那么每棵树的训练样本都是不同的，都是没有交集的，这样每棵树都是"有偏的"，都是绝对"片面的"（当然这样说可能不对），也就是说每棵树训练出来都是有很大的差异的；而随机森林最后分类取决于多棵树（弱分类器）的投票表决，这种表决应该是"求同"，因此使用完全不同的训练集来训练每棵树这样对最终分类结果是没有帮助的，这样无异于是"盲人摸象"。
```
　　2）如果每个样本的特征维度为M，指定一个常数m<<M，随机地从M个特征中选取m个特征子集，每次树进行分裂时，从这m个特征中选择最优的；

　　3）每棵树都尽最大程度的生长，并且没有剪枝过程。
```
　　一开始我们提到的随机森林中的“随机”就是指的这里的两个随机性。两个随机性的引入对随机森林的分类性能至关重要。由于它们的引入，使得随机森林不容易陷入过拟合，并且具有很好得抗噪能力（比如：对缺省值不敏感）。  

随机森林分类效果（错误率）与两个因素有关：
```
    森林中任意两棵树的相关性：相关性越大，错误率越大；
    森林中每棵树的分类能力：每棵树的分类能力越强，整个森林的错误率越低。
```
## softmax
softmax简单理解
```
首先我们简单来看看softmax是什么意思。顾名思义，softmax由两个单词组成，其中一个是max。对于max我们都很熟悉，比如有两个变量a,b。如果a>b，则max为a，反之为b。用伪码简单描述一下就是 if a > b return a; else b。 
另外一个单词为soft。max存在的一个问题是什么呢？如果将max看成一个分类问题，就是非黑即白，最后的输出是一个确定的变量。更多的时候，我们希望输出的是取到某个分类的概率，或者说，我们希望分值大的那一项被经常取到，而分值较小的那一项也有一定的概率偶尔被取到，所以我们就应用到了soft的概念，即最后的输出是每个分类被取到的概率。
```
softmax定义
```
对于每一个值，以e为底，每个值作为指数，例如输入有3、1、-3，那么其分别的变形值为e^3、e^1、e^-3,其分别的概率为这几个变形值分别除它们的累加和。
                                                     
                                                    公式为：Si=ei∑jej
```
对于softmax loss function
```
      损失函数越大，说明该分类器在真实标签上的分类概率越小，性能也就越差。
      当损失函数接近正无穷时表明训练发散，需要调小学习速率。
```
## 交叉熵
```
  给定一个策略, 交叉熵就是在该策略下猜中颜色所需要的问题的期望值。更普遍的说，交叉熵用来衡量在给定的真实分布下，使用非真实分布所指定的策略消
  
除系统的不确定性所需要付出成本的大小。交叉的字面意思在于：真实分布与非真实分布的交叉。即表示预测出来的答案与正确答案之间的错误程度。给定一个

方案, 越优的策略, 最终的交叉熵越低。具有最低的交叉熵的策略就是最优化策略，也就是上面定义的熵。因此, 在机器学习中, 我们需要最小化交叉熵。

```
数学上来讲, 交叉熵就是
```
  其中, p 是真正的概率, 例如例子二中, 橘色和绿色是 1/8, 红色是 1/4, 蓝色是 1/2。p^是错误地假设了的概率, 例如, 在例子二中我们错误地假设了所有
  
的颜色的概率都是 1/4。p和 p^ 可能有点容易混淆. 记住一点, log是用来计算在 你的策略下猜中所需要的问题数, 因此, log中需要的是你的预测概率p^。在

决策树中, 如果建立的树不是最优的, 结果就是对于输出的概率分布的假设是错误地, 导致的直接结果就是交叉熵很高。交叉熵不仅仅应用在决策树中, 在其他的

分类问题中也有应用。
```
## 正则化项
L1和L2是正则化项，又叫做罚项，是为了限制模型的参数，防止模型过拟合而加在损失函数后面的一项。    
区别：
```
    1.L1是模型各个参数的绝对值之和。

　　　L2是模型各个参数的平方和的开方值。

　　2.L1会趋向于产生少量的特征，而其他的特征都是0.

　　　因为最优的参数值很大概率出现在坐标轴上，这样就会导致某一维的权重为0 ，产生稀疏权重矩阵

　　  L2会选择更多的特征，这些特征都会接近于0。  

      最优的参数值很小概率出现在坐标轴上，因此每一维的参数都不会是0。当最小化||w||时，就会使每一项趋近于0
```
## 模型评价
  TP: True Positive---->将正类预测为正类     
  FN: False Negative---->将正类预测为负类    
  FP: False Positive---->将负类预测为正类    
  TN: True Negative---->将负类预测为负类   
准确率（ACC）: (TP+TN)/(TP+FN+FP+TN)    
灵敏度（SENS）/召回率：TP/(TP+FN)---->所有正类中预测为正类的   
特异度（SPEC）：TN/(FP+TN)---->所有负类中预测为负类的   
ROC曲线代表的意义：
```
   基本的结论：ROC曲线在斜对角线以下，则表示该分类器效果差于随机分类器，反之，效果好于随机分类器，当然，我们希望ROC曲线尽量除于斜对角线以
   
上，也就是向左上角（0,1）凸。
```
即对于ROC曲线，越是上凸表明效果越好，越是下凹表明效果越差。

## 成本敏感分析
不同类的预测错误成本不同   
例：
1.建立成本矩阵
```
如下，由于主对角线表示预测正确，因此成本为0.其他地方的成本依靠具体情况而定，这里我们设置都为1.
                  预测类                          预测类
                 yes      no                       a      b       c
真实类     yes    0        1      真实类      a    0      1       1
            no    1        0                  b    1      0       1 
                                              c    1      1       0
```
2、在预测的时候我们与概率向量相乘，选择期望成本最低的预测  
依具体情况，合适的成本矩阵且在合适时候使用将提升效果  
```
i.在训练时忽略，预测阶段考虑
ii.在预测阶段忽略，训练阶段考虑
iii.都考虑
```
## 图像局部纹理特征——HOG（Histogram of Oriented Gradient）
  HOG（方向梯度直方图）特征是一种在计算机视觉和图像处理中用来进行物体检测的特征描述子。它通过计算和统计图像局部区域的梯度方向直方图来构成特征。
  Dalal提出的Hog特征提取的过程：把样本图像分割为若干个像素的单元（cell），把梯度方向平均划分为9个区间（bin），在每个单元里面对所有像素的梯度方向在各个方向区间进行直方图统计，得到一个9维的特征向量，每相邻的4个单元（cell）构成一个块（block），把一个块内的特征向量联起来得到36维的特征向量，用块对样本图像进行扫描，扫描步长为一个单元(block_stride = 8)。最后将所有块的特征串联起来，就得到了人体的特征。例如，对于64 * 128的图像而言，每16 * 16的像素组成一个cell，每2 * 2个cell组成一个块，因为每个cell有9个特征，所以每个块内有4 * 9=36个特征，以8个像素为步长，那么，水平方向将有7个扫描窗口，垂直方向将有15个扫描窗口。也就是说，64 * 128的图片，总共有36 * 7 * 15=3780个特征。
总体流程图如下：
![rongqi](https://github.com/wls860707495/Deep-Learning/blob/master/img/hog.png)
## 局部二值模式（Local Binary Patterns）纹理灰度与旋转不变性
### 基本LBP
LBP（Local Binary Pattern，局部二值模式）是一种用来描述图像局部纹理特征的算子；它具有旋转不变性和灰度不变性等显著的优点。  
原始的LBP算子定义为在3 * 3的窗口内，以窗口中心像素为阈值，将相邻的8个像素的灰度值与其进行比较，若周围像素值大于中心像素值，则该像素点的位置被标记为1，否则为0。这样，3 * 3邻域内的8个点经比较可产生8位二进制数（通常转换为十进制数即LBP码，共256种），即得到该窗口中心像素点的LBP值，并用这个值来反映该区域的纹理信息。如下图所示
![rongqi](https://github.com/wls860707495/Deep-Learning/blob/master/img/LBP.png)
### 旋转不变的LBP
从 LBP 的定义可以看出，LBP 算子是灰度不变的，但却不是旋转不变的。图像的旋转就会得到不同的 LBP值。   
　　Maenpaa等人又将 LBP 算子进行了扩展，提出了具有旋转不变性的 LBP 算子，即不断旋转圆形邻域得到一系列初始定义的 LBP 值，取其最小值作为该邻域的 LBP 值。   
　　图 2.5 给出了求取旋转不变的 LBP 的过程示意图，图中算子下方的数字表示该算子对应的 LBP 值，图中所示的 8 种 LBP模式，经过旋转不变的处理，最终得到的具有旋转不变性的 LBP 值为 15。也就是说，图中的 8 种 LBP 模式对应的旋转不变的 LBP 模式都是00001111。  
 ![rongqi](https://github.com/wls860707495/Deep-Learning/blob/master/img/%E6%97%8B%E8%BD%AC%E4%B8%8D%E5%8F%98LBP.png)
### 旋转不变的均匀二值模式（ULBP）
　　一个LBP算子可以产生不同的二进制模式，对于半径为R的圆形区域内含有P个采样点的LBP算子将会产生2P种模式。很显然，随着邻域集内采样点数的增加，二进制模式的种类是急剧增加的。例如：5×5邻域内20个采样点，有220＝1,048,576种二进制模式。如此多的二值模式无论对于纹理的提取还是对于纹理的识别、分类及信息的存取都是不利的。同时，过多的模式种类对于纹理的表达是不利的。例如，将LBP算子用于纹理分类或人脸识别时，常采用LBP模式的统计直方图来表达图像的信息，而较多的模式种类将使得数据量过大，且直方图过于稀疏。因此，需要对原始的LBP模式进行降维，使得数据量减少的情况下能最好的代表图像的信息。   
　　为了解决二进制模式过多的问题，提高统计性，Ojala提出了采用一种“等价模式”（Uniform Pattern）来对LBP算子的模式种类进行降维。Ojala等认为，在实际图像中，绝大多数LBP模式最多只包含两次从1到0或从0到1的跳变。因此，Ojala将“等价模式”定义为：当某个LBP所对应的循环二进制数从0到1或从1到0最多有两次跳变时，该LBP所对应的二进制就称为一个等价模式类。如00000000（0次跳变），00000111（只含一次从0到1的跳变），10001111（先由1跳到0，再由0跳到1，共两次跳变）都是等价模式类。除等价模式类以外的模式都归为另一类，称为混合模式类，例如10010111（共四次跳变）（这是我的个人理解，不知道对不对）。  
　　通过这样的改进，二进制模式的种类大大减少，而不会丢失任何信息。模式数量由原来的2P种减少为 P ( P-1)+2种，其中P表示邻域集内的采样点数。对于3×3邻域内8个采样点来说，二进制模式由原始的256种减少为58种，这使得特征向量的维数更少，并且可以减少高频噪声带来的影响。
## 新的数据扩展方法


参考博客：
https://blog.csdn.net/qq_34106574/article/details/82016442   
https://blog.csdn.net/bitcarmanlee/article/details/82320853   
https://blog.csdn.net/qq_42714369/article/details/92600543   
https://www.jianshu.com/p/863b18e54d9c   
https://blog.csdn.net/jie310600/article/details/84926856  
https://blog.csdn.net/heli200482128/article/details/79204008  
