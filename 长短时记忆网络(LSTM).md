# 长短时记忆网络
为了解决循环神经网络中梯度消失和梯度爆炸的问题，是对循环神经网络的改进。  
对于长短时记忆网络的思想：原始RNN的隐藏层仅有一个状态，即h，它对于短期的状态很敏感，那么，我们再增加一个状态c，使c保存长期的状态即可解决问题。  
即它将循环神经网络隐藏层中的小圆圈换成了LSTM独有的结构，以便于筛选重要以及不重要的信息。
## 思想细节--->图未加，后加
在t时刻，LSTM的输入有三个：当前时刻网络的输入值xt、上一时刻LSTM的输出值ht-1、以及上一时刻的单元状态ct-1；LSTM的输出有两个：当前时刻LSTM输出值ht、和当前时刻的单元状态ct。注意x、h、c都是向量。此处，h与c均为同一层神经元之间的参数传递，比循环神经网络多出了一个c，x是从上一层传来的值。    
LSTM的关键，就是怎样控制长期状态c。在这里，LSTM的思路是使用三个控制开关。第一个开关，负责控制继续保存长期状态c；第二个开关，负责控制把即时状态输入到长期状态c；第三个开关，负责控制是否把长期状态c作为当前的LSTM的输出。  
## 长短时记忆网络的前向计算
所谓实现的开关即为门-->一层全连接层
它的输入是一个向量，输出是一个0到1之间的实数向量。假设W是门的权重向量，是偏置项，那么门可以表示为：
```
                                g(x) = sigmoid(W*x+b)
```
门的使用，就是用门的输出向量按元素乘以我们需要控制的那个向量。因为门的输出是0到1之间的实数向量，那么，当门输出为0时，任何向量与之相乘都会得到0向量，这就相当于啥都不能通过；输出为1时，任何向量与之相乘都不会有任何改变，这就相当于啥都可以通过。因为（也就是sigmoid函数）的值域是(0,1)，所以门的状态都是半开半闭的。  
LSTM用两个门来控制单元状态c的内容，一个是遗忘门（forget gate），它决定了上一时刻的单元状态ct-1有多少保留到当前时刻ct；另一个是输入门（input gate），它决定了当前时刻网络的输入xt有多少保存到单元状态ct。LSTM用输出门（output gate）来控制单元状态ct有多少输出到LSTM的当前输出值ht。  
遗忘门：Wf是遗忘门的权重矩阵，[ht-1,xt]表示把两个向量连接成一个更长的向量，bf是遗忘门的偏置项，（-->不会打）是sigmoid函数。如果输入的维度是dx，隐藏层的维度是dh，单元状态的维度是dc（通常），则遗忘门的权重矩阵Wf维度是dc * (dh+dx)。事实上，权重矩阵Wf都是两个矩阵拼接而成的：一个是Wfh，它对应着输入项ht-1，其维度为dc * dh；一个是Wfx，它对应着输入项xt，其维度为dc * dx。  
输入门：输入们的总输出值不只与遗忘门一样，其还要考虑当前输入的单元状态·ct--->即其输入门的输出值有两个部分组合而成，与遗忘门一样，第一部分同样是对于其输出的计算，权重矩阵与h和x的整合向量相乘+其偏置项。  
第二部分则是对于当前输入单元状态·ct的计算，其是根据上一次的输出和本次的输入来计算的。一般为tanh（Wc*[ht-1,xt]+bc）,与上一部分不同的除了偏置项之外，第一部分是用singmod函数来进行计算，这一部分使用tanh函数来计算。--->此部分不属于输入门，但是是计算ct所需要的一部分     
最终状态当前时刻单元ct是由上一次的单元状态ct-1按元素乘以遗忘门ft，再用当前输入的单元状态·ct按元素乘以输入门it，再将两个积加和产生的： 
```
                              ct = ft o ct-1 + it o ~ct              //o表示按元素乘
```
这样，我们就把LSTM关于当前的记忆·ct和长期的记忆ct-1组合在一起，形成了新的单元状态ct。由于遗忘门的控制，它可以保存很久很久之前的信息，由于输入门的控制，它又可以避免当前无关紧要的内容进入记忆。  
输出门：控制了长期记忆对当前输出的影响。Ot，计算与前两个门类似。  
LSTM最终的输出，是由输出门和单元状态共同确定的，即：
```
                             ht = Ot o tanh(ct)
```
## 长短时记忆网络的训练
此处其余反向传播训练基本一样，但是再LSTM处，因为有ft（遗忘门）、it（输入门）、ct（当前时刻输入状态）、ot（输出门），我们希望向上一层传递一个误差项，所以对于t时刻的误差项&t我们不是对其加权输入求偏导，而是对LSTM的输出ht求偏导。  
对于长短时记忆网络的训练，其误差项与RNN一样，分为沿时间传播的误差项以及向上一层传播的误差项两种。--->此处公式未完全推导，后加  
其权重梯度的计算也会分为ft（遗忘门）、it（输入门）、ct（当前时刻输入状态）、ot（输出门）四个部分，其各自的权重可根据相对应的误差项进行各自的推导即可，同时其某时刻的权重梯度也是在此时刻各自误差算出的梯度之和。  
## 总结
对于LSTM，相对于RNN来讲，增加的是使用三个门来控制长短期的信息存储，以此来解决梯度消失和梯度爆炸问题。  

## 附
对于长短时记忆网络，其上为最基础的一种，对于其，比较出名的是GRU。GRU简化了LSTM网络并且还与LSTM保持着相同的效果。其中，GRU对LSTM做了两个较大的改动。 ```
1、将输入门、遗忘门、输出门变为两个门：更新门以及重置门；
2、将单元状态与输出合并为一个状态：h；
```


