# 递归神经网络
循环神经网络虽然能够处理时间的序列，但是对于一些如树、图等复杂的结构来说，循环神经网络便不能对其进行处理。而递归神经网络(Recursive Neural Network, RNN，便可以处理这种复杂的递归结构。其训练算法为BPTS(Back Propagation Through Structure)。
## 关于递归神经网络
一句话有两个语义，循环神经网络不能处理，而使用递归神经网络则可以较好的解决这个问题。  
递归神经网络可以把一个树/图结构信息编码为一个向量，也就是把信息映射到一个语义向量空间中。这个语义向量空间满足某类性质，比如语义相似的向量距离更近。也就是说，如果两句话（尽管内容不同）它的意思是相似的，那么把它们分别编码后的两个向量的距离也相近；反之，如果两句话的意思截然不同，那么编码后向量的距离则很远。  
递归神经网络是一种表示学习，它可以将词、句、段、篇按照他们的语义映射到同一个向量空间中，也就是把可组合（树/图结构）的信息表示为一个个有意义的向量。比如上面这个例子，递归神经网络把句子"the country of my birth"表示为二维向量[1,5]。有了这个『编码器』之后，我们就可以以这些有意义的向量为基础去完成更高级的任务（比如情感分析等）。如下图所示，递归神经网络在做情感分析时，可以比较好的处理否定句，这是胜过其他一些模型的。  
尽管递归神经网络具有更为强大的表示能力，但是在实际应用中并不太流行。其中一个主要原因是，递归神经网络的输入是树/图结构，而这种结构需要花费很多人工去标注。想象一下，如果我们用循环神经网络处理句子，那么我们可以直接把句子作为输入。然而，如果我们用递归神经网络处理句子，我们就必须把每个句子标注为语法解析树的形式，这无疑要花费非常大的精力。很多时候，相对于递归神经网络能够带来的性能提升，这个投入是不太划算的。
## 递归神经网络的前向计算
对于递归神经网络，其输入为两个子节点，输出是这两个子节点编码后产生的父节点，父节点的维度和每个子节点是相同的。这其中，子节点和父节点之间组成一个全连接网络，即子节点的每个神经元与父节点的每个神经元之间两两相连。我们把产生的父节点的向量和其他子节点的向量再次作为网络的输入，再次产生它们的父节点。如此递归下去，直至整棵树处理完毕。最终，我们将得到根节点的向量，我们可以认为它是对整棵树的表示，这样我们就实现了把树映射为一个向量。递归网络前向计算公式如下：
```
                                        p = tanh(W[c1,c2] + b)
```
其与全连接神经网络的计算没有什么区别，只是在输入的过程中需要根据输入的树结构依次输入每个子节点。需要注意的是，递归网络的权重W和偏置项b在所有的节点都是共享的。  
## 递归网络的训练
归神经网络的训练算法和循环神经网络类似，两者不同之处在于，前者需要将残差&从根节点反向传播到各个子节点，而后者是将残差&从当前时刻tk反向传播到初始时刻t1。
### 误差项的传递
先求出误差函数E相对于父节点p的加权输入netp的导数。  
最终我们可以得出父节点传递到子节点的公式：  
```
                                        &c = WT &p o f'(netcj)                     //其中WT表示权重矩阵W的转置
```
由此，我们可以根据树形结构反向传递误差来推导出逐层误差的传递。---->未推导，后加
### 权重梯度的计算
首先根据加权输入的计算，我们先求出针对一个权重项wji的梯度，之后将其扩展为对所有权重项的公式。--->未写，后加。求出第l层权重项的梯度计算公式之后，由于权重  W是在所有层共享的，那么递归网络的最终权重梯度之和亦为各个层的权重梯度之和。最后对于偏置项的梯度也为误差函数对偏置项的偏导。  
### 权重更新
比如使用梯度下降优化算法，与上几节是相同思想的，偏置项的更新也一样。  





