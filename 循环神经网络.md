# 循环神经网络
当我们使用卷积神经网络以及简单的全连接神经网络时，单个神经元的的输入与上一个神经元的输入并没有关系。但是我们做的一些任务需要处理一些序列。  
     
     比如：理解一句话的意思需要将所有的词连接起来。
除此之外，我们在处理视频序列时也不应只是单独分析每一帧  
其最先用于语言模型-->自然语言处理 中。
## 语言模型
语言模型是对一种语言的特征进行建模，它有很多很多用处。比如在语音转文本(STT)的应用中，声学模型输出的结果，往往是若干个可能的候选词，这时候就需要语言模型来从这些候选词中选择一个最可能的。当然，它同样也可以用在图像到文本的识别中(OCR)。  
对于对自然语言进行处理，一般均是以向前（或向后）看多少个词来预测需要填什么。而RNN理论上可以向前（或向后）看任意个词。

## 对于循环神经网络
循环神经网络的隐藏层的值s不仅仅取决于当前这次的输入x，还取决于上一次隐藏层的值s。权重矩阵 W就是隐藏层上一次的值作为这一次的输入的权重。  
```
                              Ot = g(V st) ---> 式1
                              st = f(U xt + W st-1) ---> 式2
```
式1是输出层的计算公式，输出层是一个全连接层，也就是它的每个节点都和隐藏层的每个节点相连。V是输出层的权重矩阵，g是激活函数。式2是隐藏层的计算公式，它是循环层。U是输入x的权重矩阵，W是上一次的值作为这一次的输入的权重矩阵，f是激活函数。

从上面的公式我们可以看出，循环层和全连接层的区别就是循环层多了一个权重矩阵 W。
将式2带入式1可知：，循环神经网络的输出值ot，是受前面历次输入值xt、xt-1、xt-2、xt-3、...影响的，这就是为什么循环神经网络可以往前看任意多个输入值的原因。
## 双向循环网络
如图（图未画），双向卷积神经网络的隐藏层要保存两个值，一个A参与正向计算，另一个值A'参与反向计算。最终的输出值y2取决于A和A'。  
仿照式1和式2，可以写出双向循环神经网络的计算方法：
```
                            Ot = g(V st + V' s't)
                            st = f(U xt + W st-1)
                            s't = f(U' xt + W' s't+1)
```
从上面三个公式可以看出，正向和反向计算不共享权重，也就是说U和U'、W和W'、V和V'都是不同的权重矩阵。
## 深度循环神经网络
即堆叠两个以上隐藏层的深度循环神经网络。
## 循环神经网络的训练
### 循环神经网络的训练算法：BPTT
BPTT算法是针对循环层的训练算法，它的基本原理和BP算法是一样的，也包含同样的三个步骤：

1、前向计算每个神经元的输出值；
2、反向计算每个神经元的误差项值，它是误差函数E对神经元j的加权输入的偏导数；
3、计算每个权重的梯度。
最后再用随机梯度下降算法更新权重。
#### 误差项的计算
BTPP算法将第l层t时刻的误差项&(l t)值沿两个方向传播，一个方向是其传递到上一层网络，得到&(l-1 t)，这部分只和权重矩阵U有关；另一个是方向是将其沿时间线传递到初始t1时刻，得到&l1，这部分只和权重矩阵W有关。



