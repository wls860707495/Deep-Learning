# 循环神经网络
当我们使用卷积神经网络以及简单的全连接神经网络时，单个神经元的的输入与上一个神经元的输入并没有关系。但是我们做的一些任务需要处理一些序列。  
     
     比如：理解一句话的意思需要将所有的词连接起来。
除此之外，我们在处理视频序列时也不应只是单独分析每一帧  
其最先用于语言模型-->自然语言处理 中。
## 语言模型
语言模型是对一种语言的特征进行建模，它有很多很多用处。比如在语音转文本(STT)的应用中，声学模型输出的结果，往往是若干个可能的候选词，这时候就需要语言模型来从这些候选词中选择一个最可能的。当然，它同样也可以用在图像到文本的识别中(OCR)。  
对于对自然语言进行处理，一般均是以向前（或向后）看多少个词来预测需要填什么。而RNN理论上可以向前（或向后）看任意个词。

## 对于循环神经网络
循环神经网络的隐藏层的值s不仅仅取决于当前这次的输入x，还取决于上一次隐藏层的值s。权重矩阵 W就是隐藏层上一次的值作为这一次的输入的权重。  
```
                              Ot = g(V st) ---> 式1
                              st = f(U xt + W st-1) ---> 式2
```
式1是输出层的计算公式，输出层是一个全连接层，也就是它的每个节点都和隐藏层的每个节点相连。V是输出层的权重矩阵，g是激活函数。式2是隐藏层的计算公式，它是循环层。U是输入x的权重矩阵，W是上一次的值作为这一次的输入的权重矩阵，f是激活函数。

从上面的公式我们可以看出，循环层和全连接层的区别就是循环层多了一个权重矩阵 W。
将式2带入式1可知：，循环神经网络的输出值ot，是受前面历次输入值xt、xt-1、xt-2、xt-3、...影响的，这就是为什么循环神经网络可以往前看任意多个输入值的原因。
## 双向循环网络
如图（图未画），双向卷积神经网络的隐藏层要保存两个值，一个A参与正向计算，另一个值A'参与反向计算。最终的输出值y2取决于A和A'。  
仿照式1和式2，可以写出双向循环神经网络的计算方法：
```
                            Ot = g(V st + V' s't)
                            st = f(U xt + W st-1)
                            s't = f(U' xt + W' s't+1)
```
从上面三个公式可以看出，正向和反向计算不共享权重，也就是说U和U'、W和W'、V和V'都是不同的权重矩阵。
## 深度循环神经网络
即堆叠两个以上隐藏层的深度循环神经网络。
## 循环神经网络的训练
